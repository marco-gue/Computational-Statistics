---
title: "Homework 5"
subtitle: "Prof: Luiz Peternelli"
author: "Marco Guerra Hinostroza"
date: "`r Sys.Date()`"
format:
  html:
    embed-resources: true
    toc: true
    toc-location: left
    number-sections: false
    code-fold: false
    filters:
        - highlight-text
editor_options: 
  chunk_output_type: console
---

```{r, include=FALSE}
library(psych)
library(ggplot2)
```


## Question 2

#### Probability density function:

$$
f(x, y, z) = 
\begin{cases}
k(x + y + z) \quad ; \quad 0 \le x \le 1 \quad 0 \le y \le 1 \quad 0 \le z \le 1\\
0 \quad ; \quad c.c
\end{cases}
$$

Since the area under the curve is different from 1
$$
b \int_0^1 \int_0^1 \int_0^1 k(x+y+z) \, dz\,dy\,dx = 1
$$

Integrando en relación a z
$$
\int_0^1 k(x+y+z) \, dz = k\int_0^1 (x+y+z) \, dz =
$$

$$
 k \left[ (x+y)z + \frac{z^2}{2} \right]^1_0 = k\left[ x+y + \frac{1}{2} \right]
$$

Integrando en relación a y

$$
\int_0^1 k\left[ x+y + \frac{1}{2} \right] \, dy = k\int_0^1 \left[ x+y + \frac{1}{2} \right] \, dy =
$$

$$
 k \left[ xy + \frac{y^2}{2} + \frac{y}{2} \right]^1_0 = k\left[ x+ \frac{1}{2} + \frac{1}{2} \right] = k(x+1)
$$

Integrando en relación a x

$$
\int_0^1 k(x+1)\, dx = k\int_0^1(x+1)\, dx = k \left[ \frac{x^2}{2} + x  \right]^1_0 = k \left[ \frac{1}{2} + 1  \right] = k \times \frac{3}{2}
$$

Agora, fazemos essa integral igual a 1:

$$
k \times \frac{3}{2} = 1 \Rightarrow k = \frac{2}{3}
$$

#### f.d.p marginal de X

$$
 f(x) = \int_0^1 \int_0^1  f(x+y+z) \, dz\,dy \, =  \, \int_0^1 \int_0^1  \frac{2}{3}(x+y+z) \, dz\,dy
$$

Integramos em relação a z no intervalo [0,1]:

$$
\frac{2}{3}\int_0^1(x+y+z) \, dz = \frac{2}{3} \left[ (x+y)z + \frac{z^2}{2}  \right]^1_0 = \frac{2}{3} \left[ x+y + \frac{1}{2}  \right]
$$

Integramos em relação a y no intervalo [0,1]:

$$
\frac{2}{3}\int_0^1(x+y+ \frac{2}{3}) \, dy = \frac{2}{3} \left[ xy + \frac{y^2}{2} + \frac{y}{2} \right]^1_0 = \frac{2}{3}(x+1)
$$

A f.d.p. marginal de X é:

$$
f(x) = \frac{2}{3}(x+1), \quad 0 \le x \le 1
$$

#### f(x , y)

$$
f(x,y) = \int_0^1f(x,y,z)\, dz = \frac{2}{3}\int_0^1(x+y+z)dz =
$$


$$
\frac{2}{3}\left[ (x+y)z + \frac{z^2}{2} \right]^1_0 = \frac{2}{3}\left[x + y + \frac{1}{2} \right]
$$

$$
f(x,y) = \frac{2}{3}(x + y + \frac{1}{2})
$$


#### f(y | x)

$$
f(y|x) = \frac{f(x,y)}{f(x)}
$$

$$
f(y|x) = \frac{\frac{2}{3}(x+y+ \frac{1}{2})}{\frac{2}{3}(x+1)} = \frac{x+y+\frac{1}{2}}{x+1}
$$

#### f(z | x,y)

$$
f(z|x,y) = \frac{f(x,y,z)}{f(x,y)}
$$

$$
f(y|x) = \frac{\frac{2}{3}(x+y+z)}{\frac{2}{3}(x+y+\frac{1}{2})} = \frac{x+y+z}{x+y+\frac{1}{2}}
$$

#### Distribucao acumulada F(x)

$$
F(x) = P(X \le x) = \int_0^x f(t)\, dt
$$

$$
F(x) = \int_0^x \frac{2}{3}(t+1)\,dt = \frac{2}{3} \left[ \frac{t^2}{2} + t \right]_0^x = \frac{2}{3} \left[ \frac{x^2}{2} + x  \right]
$$

$$
F(x) = \frac{1}{3}x^2 + \frac{2}{3}x
$$


#### F(y | x)

$$
F(y|x) = P(Y \le y \, | \,  X=x) = \int_0^y f(t|x)\, dt
$$

$$
F(y|x) = \int_0^y \frac{x+t+\frac{1}{2}}{x+1}\,dt =  \frac{1}{x+1} \int_0^y (x + t + \frac{1}{2})\,dt = 
$$

$$
\frac{1}{x+1} \left[ xt + \frac{t^2}{2} + \frac{t}{2} \right]^y_0 = \frac{1}{x+1} \left[ xy + \frac{y^2}{2} + \frac{y}{2} \right]
$$

$$
F(y|x) = \frac{xy + \frac{y^2}{2} + \frac{y}{2}}{x+1}
$$

#### F(z | x,y)

$$
F(z|x,y) = P(Z \le z \, | \,  X=x, Y=y) = \int_0^z f(t|x,y)\, dt
$$

$$
F(z|x,y) = \int_0^z \frac{x+y+t}{x+y+ \frac{1}{2}}\,dt =  \frac{1}{x+y + \frac{1}{2}} \int_0^z (x + y + t)\,dt = 
$$

$$
\frac{1}{x+y + \frac{1}{2}} \left[ (x+y)t + \frac{t^2}{2} \right]^z_0 = \frac{1}{x+y + \frac{1}{2}} \left[ (x+y)z + \frac{z^2}{2} \right]
$$

$$
F(z|x,y) = \frac{(x+y)z + \frac{z^2}{2}}{x +y + \frac{1}{2}}
$$


## Question 4
```{r}
X <- c(10, 20, 30, 40, 50)
p_X <- c(0.10, 0.45, 0.30, 0.10, 0.05)

plot(X, p_X, type="h", main = "Probability distribution",
     xlab="Random Variable X",ylab="Probability P(X=x)",
     font=1,font.lab=1, lwd = 2, ylim = c(0, 0.47))
```

```{r}
data <- rep(X, times = p_X * 100)

hist(data, breaks = c(5, 15, 25, 35, 45, 55), ylim = c(0, 0.05),
     freq = FALSE, col = "#4682B4", border = "black",
     main = "Histogram of probabilities",
     xlab = "Random Variable X", ylab = "Probability")
axis(1, at = seq(5, 55, by = 5), labels = seq(5, 55, by = 5))

```

## Question 6


```{r}
gera.mult.chol<-function(v.medias, sigma, iteracoes)
  {
  mu<-v.medias
  fc<-chol(sigma) #matriz da cholesky
  iter<-iteracoes
  p<-length(fc[1,])
  variaveis<-matrix(99,iter,p)
  for (i in 1:iter)
    {
    #gerar o vetor z a partir da distribuicao desejada, de dimencao px1
    z<-rnorm(p) #usando aqui a distribuicao normal(0,1)
    #obter o vetor multivariado, de dimensao px1, fazendo
    variaveis[i,]<-mu+t(fc)%*%z #mu eh um vetor de medias px1
    }
  return(variaveis)
  }
```

Define the parameters:
```{r}
u <- c(1.5, 10, 2.5) # Adjusted means for X, Y, and Z
sigma <- matrix(c(10, 2, 0.3,
                  2, 16, -0.5,
                 0.3, -0.5, 1), ncol = 3) # Covariance matrix
```


```{r}
set.seed(123)
j <- gera.mult.chol(v.medias = u, sigma = sigma, iteracoes = 1000)
cov(j)
```

:::: {.columns}

::: {.column width="40%"}
Target Covariance Matrix
```{r}
sigma
```
:::

::: {.column width="2%"}
:::

::: {.column width="58%"}
Empirical Covariance Matrix
```{r}
cov(j)
```
:::

::::

```{r}
# usar o método da transformação inversa
u1<-pnorm(j[,1])
plot(j[,1],pnorm(j[,1]))
u2<-pnorm(j[,2])

plot(j[,2],pnorm(j[,2]))
u3<-pnorm(j[,3])
plot(j[,3],pnorm(j[,3]))
# obter a matriz de v.a. uniformemente distribuidas
u<-cbind(u1,u2,u3)
head(u)
summary(u)
boxplot(u)
pairs.panels(u)
```

```{r}
#supor as novas variaveis de interesse. Fazer x = F^-1(u)
#exponencial
x1<-qexp(u1)
hist(x1,prob=T)
curve(dexp(x),0,20,add=T,col="blue")
#normal
x2<-qnorm(u2,10,4)
hist(x2,prob=T)
curve(dnorm(x,4,2),-6,15,add=T,col="blue")
#t  
# x3<-qt(u3, df = 5)
# hist(x3,prob=T)
# curve(dt(x,5),-5, 5,add=T,col="blue")
x3<-ifelse(u3<1/3,2,4) #supondo uma v.a.d onde x=2 com prob 1/3 e x=4 com prob 2/3
plot(table(x3)/length(x3))
# x3 <- sqrt(u3)
```

```{r}
breaks <- quantile(j[, 3], probs = c(0, 0.1, 0.3, 0.6, 1)) 

# Transformar os valores contínuos em categorias 1, 2, 3, 4
j[, 3] <- cut(j[, 3], breaks = breaks, labels = 1:4, include.lowest = TRUE)

# Converter para valores numéricos
j[, 3] <- as.numeric(j[, 3])
```

```{r}
j[, 1] <- pnorm(j[,1])

# Aplicar a inversa da CDF para ajustar à distribuição X ~ 3x^2
j[, 1] <- j[, 1]^(1/3)

# Ajustar os valores para o intervalo (0, 3)
j[, 1] <- j[, 1] * 3
```

## Question 9

$$
X = \{2,5,3,7,6,6,5,6,7,9\}
$$

#### Discrete empirical distribution

$$
F_n(X)=
\begin{cases}
0 \quad se \quad z < y_i\\
\frac{1}{n} \quad se \quad y_i \le x \le y_n \\
1 \quad se \quad x >y_n
\end{cases}
$$

```{r}
X <- c(2, 5, 3, 7, 6, 6, 5, 6, 7, 9)
X <- sort(X)
prob_ac <- cumsum(table(X)/length(X))
plot(ecdf(prob_ac), xlim = c(0,1),
main = "Discrete empirical distribution")
```

```{r}
sim <- NULL
for (i in 1:10000)
  {
  u <- runif(1)
  sim[i] <- as.numeric(names(which(u <= prob_ac)[1]))
}
plot(ecdf(cumsum(table(sim)/length(sim))), xlim = c(0,1),
main = "Discrete empirical distribution (10000 simulations)")
```

#### Continuous empirical distribution

$$
F_n(X)=
\begin{cases}
0 \quad ; \quad z < y_i\\
\frac{i+1}{n-1} + \frac{x - X_i}{(n-1)(X_{i+i} - X_i)} \quad ; \quad y_i \le x \le y_n \\
1 \quad ; \quad x >y_n
\end{cases}
$$

to $i = 1, 2, . . . , n − 1$

The algorithm consists of simulating the value of a uniform $[u ∼ unif(0, 1)]$ and using this value in $P = n-1\times u$, where n is the number of elements in X. From P, we obtain $\mathbf{I}$, which consists of an integer by rounding P down. The simulated value is finally obtained by making $y = X_{[I]} + (P -I + 1)(X_{[I+1]} - X_{[I]})$:

```{r}
y_sim = NULL
u = NULL
for (i in 1:10000) {
u[i] = runif(1)
P = (length(X)-1)*u[i]
I = floor(P)+1
y_sim[i] = X[I] + (P-I+1) * (X[I+1]-X[I])
}
plot(y_sim, u, main = "Continuous empirical distribution (10000 simulations)",
xlab = "Simulated values", ylab = "F(x)")
```


## Question 10

#### Chi-square ($X^2$) distribution
The distribution of the chi-squared statistic with k degrees of freedom is derived from the sum of squares of k standard normal variables $[e.g. Z ∼ N(0, 1)]$:

$$
X^2 = Z_1^2 + Z_2^2 + Z_3^2 + ... + Z_k^2 \quad ; \quad X^2 \sim X_k^2
$$

The standard function uses this property to generate values that follow this sampling distribution. Simulating, for example, 10 standard normal observations:

```{r}
iter<-1000
X<-NULL
for (i in 1:iter)
  {
  k<-10
  z<-rnorm(k)
  X[i]<-sum(z^2)
}
```

Let's check that the values generated are as desired. First, we check the histogram:

```{r, echo=FALSE, message=FALSE}
ggplot() +
  geom_histogram(aes(x = X, after_stat(density)), color = "black") +
  stat_function(fun = function(x) dchisq(x,k), color = "tomato", linewidth = 1.5) +
  ylab("Density") +
  theme_bw()
```

We can also check that the estimates of the mean and variance parameters are as expected. The expectation of the chi-squared $\mathbf{FDP}$ is equal to the number of degrees of freedom that gave rise to the distribution, while the variance corresponds to this same number multiplied by two.

:::: {.columns}

::: {.column width="40%"}
Mean
```{r}
mean(X)
```
:::

::: {.column width="2%"}
:::

::: {.column width="58%"}
Variance
```{r}
mean(X^2) - mean(X)^2
```
:::

::::

As both $E(X)$ and $V(X)$ are close to the expected value (10 and 20, respectively), we conclude that the simulation was successful in generating values that follow the sampling distribution in question.

Let's see what happens when we simulate samples from a non-standard normal. Let's consider a mean of 10 and a variance of 8:

```{r}
iter<-1000
X<-NULL
mu = 10
vari = 8
for (i in 1:iter)
  {
  k<-10
  z<-rnorm(k, mean = mu, sd = sqrt(vari))
  X[i]<-sum(z^2)
}
```

Let's see if the values follow what is expected:

```{r,echo=FALSE,message=FALSE,warning=FALSE}
ggplot() +
  geom_histogram(aes(x = X, y = after_stat(density)), color = "black") +
  stat_function(fun = function(x) dchisq(x,k), color = "tomato", linewidth = 1.5) +
  xlim(0, max(X)) +
  ylab("Density") +
  theme_bw()
```

:::: {.columns}

::: {.column width="40%"}
Mean
```{r}
mean(X)
```
:::

::: {.column width="2%"}
:::

::: {.column width="58%"}
Variance
```{r}
mean(X^2) - mean(X)^2
```
:::

::::

By deviating from the standard normality assumption, the protocol used for the simulation does not generate values that follow the chi-square distribution. However, it is possible to generate values from the chi-squared distribution using non-standard normal observations. To do so, it is sufficient to normalise the observations, i.e.

$$
\frac{\sum_{i=1}^k(x_i - \overline{x})}{\sigma^2} \sim X^2_k
$$

```{r}
iter<-1000
X<-NULL
mu <- 10
vari <- 8
for (i in 1:iter)
  {
  k<-10 
  z<-rnorm(k, mean = mu, sd = sqrt(vari))
  X[i]<-sum((z-mu)^2)/vari
}
```

```{r,echo=FALSE,message=FALSE,warning=FALSE}
ggplot() +
  geom_histogram(aes(x = X, y = after_stat(density)), color = "black") +
  stat_function(fun = function(x) dchisq(x,k), color = "tomato", linewidth = 1.5) +
  xlim(0, max(X)) +
  ylab("Density") +
  theme_bw()
```

:::: {.columns}

::: {.column width="40%"}
Mean
```{r}
mean(X)
```
:::

::: {.column width="2%"}
:::

::: {.column width="58%"}
Variance
```{r}
mean(X^2) - mean(X)^2
```
:::

::::


#### Distribution t-Student
Values that follow a t-distribution with *k* degrees of freedom can be simulated indirectly using simulated values from a standard normal distribution and a chi-squared distribution:

$$
t = \frac{Z}{\sqrt{\frac{X^2}{k}}}
$$

Using this ratio, the FDP of the t-distribution will have zero expectation and variance equal to $\frac{k}{k-2}$ for $k > 2$. Let's look at the corresponding code:

```{r}
iter <- 10000
k <- 25
z <- rnorm(iter)
chi <- rchisq(iter, df = 25)
X <- z/sqrt(chi/k)
```

```{r,echo=FALSE,message=FALSE}
ggplot() +
  geom_histogram(aes(x = X, after_stat(density)), color = "black") +
  stat_function(fun = function(x) dt(x,k), color = "tomato",linewidth = 1.5) +
  ylab("Density") +
  theme_bw()
```

:::: {.columns}

::: {.column width="40%"}
Mean
```{r}
mean(X)
```
:::

::: {.column width="2%"}
:::

::: {.column width="58%"}
Variance
```{r}
mean(X^2) - mean(X)^2
```
:::

::::

What if we used values from a non-standard normal in the denominator of t?

```{r}
iter <- 10000
k <- 25
mu <- 10
vari <- 8
z <- rnorm(iter, mean = mu, sd = sqrt(vari))
chi <- rchisq(iter, df = 25)
X <- z/sqrt(chi/k)
```

```{r,echo=FALSE,message=FALSE}
ggplot() +
  geom_histogram(aes(x = X, after_stat(density)), color = "black") +
  stat_function(fun = function(x) dt(x,k), color = "tomato", linewidth = 1.5) +
  ylab("Density") +
  theme_bw()
```

:::: {.columns}

::: {.column width="40%"}
Mean
```{r}
mean(X)
```
:::

::: {.column width="2%"}
:::

::: {.column width="58%"}
Variance
```{r}
mean(X^2) - mean(X)^2
```
:::

::::

The values are different from those expected. In this situation, you can use another estimator for the t-statistic:

$$
t = \frac{\overline{x}-u}{\frac{s}{\sqrt{k}}}
$$

In this case:

$$
t \quad ; \quad t_{k-1}
$$

```{r}
iter <- 10000
k <- 25
mu <- 10
vari <- 8
X <- NULL
for (i in 1:iter)
  {
  z <- rnorm(k, mean = mu, sd = sqrt(vari))
  X[i] <- (mean(z) - mu)/(sqrt(vari)/sqrt(k))
}
```

```{r,echo=FALSE,message=FALSE}
ggplot() +
  geom_histogram(aes(x = X, after_stat(density)), color = "black") +
  stat_function(fun = function(x) dt(x, k-1),
                color = "tomato", linewidth = 1.5) +
  ylab("Density") +
  theme_bw()
```

:::: {.columns}

::: {.column width="40%"}
Mean
```{r}
mean(X)
```
:::

::: {.column width="2%"}
:::

::: {.column width="58%"}
Variance
```{r}
mean(X^2) - mean(X)^2
```
:::

::::

The t-distribution is closely related to the normal distribution. It is interesting because it has longer and heavier ranges than the standard normal distribution, depending on the degrees of freedom. Therefore, the t-distribution is useful for simulating data with a higher frequency of extreme values. Let's see what happens if we deviate from normality when simulating the data. We'll use the Poisson distribution in this example:

```{r}
iter <- 10000
k <- 25
mu <- 10
vari <- 8
X <- NULL
for (i in 1:iter)
  {
  z = rpois(k, lambda = mu)
  X[i] = (mean(z) - mu)/(sqrt(vari)/sqrt(k))
}
```

```{r,echo=FALSE,message=FALSE}
ggplot() +
  geom_histogram(aes(x = X, after_stat(density)), color = "black") +
  stat_function(fun = function(x) dt(x, k-1), color = "tomato", linewidth = 1.5) +
  ylab("Density") +
  theme_bw()
```

:::: {.columns}

::: {.column width="40%"}
Mean
```{r}
mean(X)
```
:::

::: {.column width="2%"}
:::

::: {.column width="58%"}
Variance
```{r}
mean(X^2) - mean(X)^2
```
:::

::::

In this example, the t-distribution resembled the Poisson distribution with $\mathbf{\lambda} = 10$. In this distribution, it is assumed that $E(X) = V(X) = \mathbf{\lambda}$.  In this example, the use of the Poisson distribution will not harm the inferences.

#### Distribution F-Snedecor
The F distribution is a chi-squared ratio:

$$
F = \frac{\frac{X_u^2}{u}}{\frac{X_v^2}{v}} \quad \rightarrow \quad F \quad \sim \quad F_{\frac{u}{v}}
$$

To simulate values from a $\mathbf{F}$ distribution, just simulate values from two chi-squares and calculate the ratio between them:

```{r, message=FALSE}
iter <- 10000
u <- 25
v <- 20
chi.u <- rchisq(iter, u)
chi.v <- rchisq(iter, v)
Fcalc <- (chi.u/u)/(chi.v/v)
```


```{r,echo=FALSE,message=FALSE}
ggplot() +
  geom_histogram(aes(x = Fcalc, after_stat(density)), color = "black") +
  stat_function(fun = function(x) df(x, u-1, v-1),
                color = "tomato", linewidth = 1.5) +
  ylab("Density") +
  theme_bw()

ks.test(Fcalc, "pf", df1 = u - 1, df2 = v - 1)
```


In the case of normal variables. F-distribution of the ratio of variances of different samples from the same population, it is known that $\quad$ $\sigma^2 \quad \sim \quad \frac{\sigma}{k-1}X^2_{k-1}$

```{r, message=FALSE}
iter <- 10000
n1 <- 100
var1 <- 10
n2 <- 120
var2 <- 10
Fcalc <- NULL

for (i in 1:iter) {
  amostra1 <- rnorm(n1, 0, sqrt(var1))
  amostra2 <- rnorm(n2, 0, sqrt(var2))
  Fcalc[i] <- var(amostra1) / var(amostra2)
}
```


```{r,echo=FALSE,message=FALSE}
ggplot() +
  geom_histogram(aes(x = Fcalc, after_stat(density)), color = "black") +
  stat_function(fun = function(x) df(x, n1-1, n2-1),
                color = "tomato", linewidth = 1.5) +
  ylab("Density") +
  theme_bw()

ks.test(Fcalc, "pf", df1 = n1 - 1, df2 = n2 - 1)
```


What if we deviate from normality? Let's test with the Weibull distribution, using two samples with the same shape parameter, so that:

```{r, message=FALSE}
set.seed(25)
iter <- 10000
n1 <- 100
n2 <- 120
Fcalc <- NULL
for (i in 1:iter)
  {
  weibull1 <- rweibull(n1, shape = 5, scale = 20)
  weibull2 <- rweibull(n2, shape = 5, scale = 20)
  Fcalc[i] <- var(weibull1)/var(weibull2)
}
```

```{r,echo=FALSE,message=FALSE}
ggplot() +
  geom_histogram(aes(x = Fcalc, after_stat(density)), color = "black") +
  stat_function(fun = function(x) df(x, n1-1, n2-1),
                color = "tomato",linewidth = 1.5) +
  ylab("Density") +
  theme_bw()

ks.test(Fcalc, "pf", df1 = n1 - 1, df2 = n2 - 1)
```

Interestingly, when the shape parameter of the Weibull distribution is equal to 3, the values generated by simulation follow a distribution F