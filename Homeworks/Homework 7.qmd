---
title: "Homework 7"
subtitle: "Prof: Luiz Peternelli"
author: "Marco Guerra Hinostroza"
date: "`r Sys.Date()`"
format:
  html:
    embed-resources: true
    toc: true
    toc-location: left
    number-sections: false
    code-fold: false
    filters:
        - highlight-text
editor_options: 
  chunk_output_type: console
---
```{r, include=FALSE}
library(ggplot2)
```


## Question 1

### A. Randomization Test
```{r}
resample_u_between <- function(data, n1, n2, iterations) {
  
  nsample <- n1 + n2
  
  # Calcula a diferença observada (target)
  target <- abs(mean(data[1:n1]) - mean(data[(n1 + 1):nsample]))
  
  # Inicializa vetor para armazenar comparações
  ucomp <- numeric(iterations)
  
  # Loop para realizar permutações
  for (y in 1:iterations) {
    # Permutação dos dados
    perm <- sample(data, replace = FALSE)
    
    # Divide os dados permutados em duas amostras
    mean_first <- mean(perm[1:n1])
    mean_second <- mean(perm[(n1 + 1):nsample])
    
    # Calcula a diferença absoluta
    mean_diff <- abs(mean_first - mean_second)
    
    # Compara a diferença permutada com a observada
    ucomp[y] <- ifelse(mean_diff >= target, 1, 0)
  }
  
  # Calcula a proporção de diferenças permutadas maiores ou iguais à observada
  uexceed <- mean(ucomp)
  
  # Exibe os resultados
  cat("Diferença média observada:", target, "\n")
  cat("Proporção excedendo a diferença média observada:", uexceed, "\n")
}

```

#### Normal
```{r}
popn_20 <- rnorm(20,10,1)
resample_u_between(popn_20, 10, 10, 10000)
```

```{r}
popn_60 <- rnorm(60,10,1)
resample_u_between(popn_60, 30, 30, 10000)
```

```{r}
popn_100 <- rnorm(100,10,1)
resample_u_between(popn_100, 50, 50, 10000)
```


#### Poisson
```{r}
popp_20 <- rpois(20,10)
resample_u_between(popp_20, 10, 10, 10000)
```


```{r}
popp_60 <- rpois(60,10)
resample_u_between(popp_60, 30, 30, 10000)
```


```{r}
popp_100 <- rpois(100,10)
resample_u_between(popp_100, 50, 50, 10000)
```

### B. Booststrap for IC

Assuming normality, the approximate 95\% confidence interval is given by

$$
\widehat{CV} \pm 1.96\sqrt{Var(bootstrap)}
$$

```{r, eval=FALSE}
cv <- function(x) sqrt(var(x))/mean(x)

boot <-NA

for (i in 1:10000) boot[i] <- cv(sample(x,replace=T))

### Recall from the notes that the estimate of the bias
# is given by the difference between the mean of the bootstrap values
# and the initial estimate,

bias <- mean(boot) - cv(x)

cv(x) - bias - 1.96*sqrt(var(boot))

cv(x) - bias + 1.96*sqrt(var(boot))
```

#### Normal

```{r, echo=FALSE}
cv <- function(x) sqrt(var(x))/mean(x)

lista_de_dados <- list(popn_20,popn_60,popn_100)

for (k in 1:length(lista_de_dados)) {
  x <- lista_de_dados[[k]] 

  boot <- NA
  for (i in 1:10000) {
    boot[i] <- cv(sample(x, replace = TRUE))
  }

  bias <- mean(boot) - cv(x)

  lower_bound <- cv(x) - bias - 1.96 * sqrt(var(boot))
  upper_bound <- cv(x) - bias + 1.96 * sqrt(var(boot))

  # Exibir resultados para o conjunto de dados atual
  cat("Para o conjunto de dados com distribuição Normal", k, ":\n")
  cat("CV:", cv(x), "\n")
  cat("Viés:", bias, "\n")
  cat("Limite inferior do intervalo de confiança:", lower_bound, "\n")
  cat("Limite superior do intervalo de confiança:", upper_bound, "\n\n")
}
```

#### Poisson

```{r, echo=FALSE}
cv <- function(x) sqrt(var(x))/mean(x)

lista_de_dados <- list(popp_20,popp_60,popp_100)

for (k in 1:length(lista_de_dados)) {
  x <- lista_de_dados[[k]] 

  boot <- NA
  for (i in 1:10000) {
    boot[i] <- cv(sample(x, replace = TRUE))
  }

  bias <- mean(boot) - cv(x)

  lower_bound <- cv(x) - bias - 1.96 * sqrt(var(boot))
  upper_bound <- cv(x) - bias + 1.96 * sqrt(var(boot))

  # Exibir resultados para o conjunto de dados atual
  cat("Para o conjunto de dados com distribuição Poisson  ", k, ":\n")
  cat("CV:", cv(x), "\n")
  cat("Viés:", bias, "\n")
  cat("Limite inferior do intervalo de confiança:", lower_bound, "\n")
  cat("Limite superior do intervalo de confiança:", upper_bound, "\n\n")
}
```


## Question 2

### A. Randomization Test

#### Normal
```{r}
pop1 <- rnorm(10,10,1)
pop2 <- rnorm(10,30,5)
popn_20 <- c(pop1,pop2)
resample_u_between(popn_20, 10, 10, 10000)
```

```{r}
pop1 <- rnorm(30,10,1)
pop2 <- rnorm(30,30,5)
popn_60 <- c(pop1,pop2)
resample_u_between(popn_60, 30, 30, 10000)
```

```{r}
pop1 <- rnorm(50,10,1)
pop2 <- rnorm(50,30,5)
popn_100 <- c(pop1,pop2)
resample_u_between(popn_100, 50, 50, 10000)
```

### B. Booststrap for IC

#### Normal

```{r}
cv <- function(x) sqrt(var(x))/mean(x)

lista_de_dados <- list(popn_20,popn_60,popn_100)

for (k in 1:length(lista_de_dados)) {
  x <- lista_de_dados[[k]] 

  boot <- NA
  for (i in 1:10000) {
    boot[i] <- cv(sample(x, replace = TRUE))
  }

  bias <- mean(boot) - cv(x)

  lower_bound <- cv(x) - bias - 1.96 * sqrt(var(boot))
  upper_bound <- cv(x) - bias + 1.96 * sqrt(var(boot))

  # Exibir resultados para o conjunto de dados atual
  cat("Para o conjunto de dados com distribuição Normal", k, ":\n")
  cat("CV:", cv(x), "\n")
  cat("Viés:", bias, "\n")
  cat("Limite inferior do intervalo de confiança:", lower_bound, "\n")
  cat("Limite superior do intervalo de confiança:", upper_bound, "\n\n")
}
```

## Question 3

**k-Fold Cross-Validation**

```{r}
library(ISLR)
set.seed(17)
```

```{r}
sorteio <- sample(1:length(Auto$mpg), replace = FALSE)
```

This approach involves randomly dividing the set of observations into $k$ groups, or folds, of approximately equal size. The ﬁrst fold is treated as a validation set, and the method is ﬁt on the remaining $k − 1$ folds. The mean squared error, $MSE$, is then computed on the observations in the held-out fold. This procedure is repeated $k$ times; each time, a diﬀerent group of observations is treated as a validation set.
```{r, warning=FALSE}
k <- 10
cvs <- split(sorteio,1:k)
cv <- 1:k 
```

```{r}
grau.polinomio <- 5
cv.error.10 <- matrix (NA , nrow = k, ncol = grau.polinomio)
```

```{r}
for (k in cv) { 
  for (i in 1:grau.polinomio) {
    
    glm.fit <- glm(mpg ~ poly(horsepower ,i),data=Auto[-c(cvs[[k]]),])
    
    cv.error.10[k,i] <- mean((Auto$mpg-predict(glm.fit, Auto))[cvs[[k]]]^2, na.rm = T)
    
  }
}
```

```{r}
cv.error.10
```

```{r}
matplot(1:grau.polinomio, t(cv.error.10), type = "b", pch = 1,
        ylab = "Mean Squared Error", xlab = "Degree of Polynomial")
```


```{r}
colMeans(cv.error.10)
diag(var(cv.error.10))
```

## Question 4
```{r}
metropolis.ex1<-function(tamanho.amostra){
  
  n<-tamanho.amostra
  
  amostra.y<-numeric(n)
  amostra.z<-numeric(n)

  countn<-0

  countu<-0
  
  repeat{
    if (countn==n) break
      u1y <- runif(1)
      u2y <- runif(1,0,5/3)
      if(u2y < 1/3*(1+4*u1y)){
        amostra.y[countn+1] <- u1y
        #u1y <- 0 #se quiser fixar um valor de y = u1y
        yi <- amostra.y[countn+1]

        v <- runif(1) #da fv(.) #aqui escolhi usar fv = uniforme
        amostra.z[1] <- v #esse é o meu z0
        ui <- runif(1)
        vi <- runif(1)
        zimenos1 <- amostra.z[countn+1]
        fyvi <- 2*(vi+2*yi)/(1+4*yi)
        fyzimenos1 <- 2*(zimenos1+2*yi)/(1+4*yi)
        # agora, no caso, como a fv é uniforme no intervalo (0, 1)
        fvvi <- 1
        fvzimenos1 <- 1
        roi <- min((fyvi/fvvi*fvzimenos1/fyzimenos1),1)
        if(ui <= roi){amostra.z[countn+2]<-vi}
        else{amostra.z[countn+2]<-zimenos1}
        # fim do Metropolis-Hasting
        countn <- countn+1
        }
      countu <- countu+1
  }
  
  cat("foram gerados ",countu, "grupos de v.a. uniformes","\n")
  cat("para simular ",countn, "valores das v.a.s de interesse","\n")
  cat("razão de eficiência aproximada", round((countn/countu)*100,2), "%","\n")
  
  return(list(y=amostra.y, z=amostra.z[-1] ))
  
}
```

#### Acceptance-Rejection Sampling for $\mathbf{Y}$
```{r}
u1y <- runif(1)
u2y <- runif(1, 0, 5/3)
```
Generates two uniform random numbers:

* [u1y]{colour="#FFFFFF" bg-colour="#009999"}: Uniformily distributed in $[0,1]$

* [u2y]{colour="#FFFFFF" bg-colour="#009999"}: Uniformily distributed in $\left[0,\frac{5}{3}\right]$

```{r, eval=FALSE}
if (u2y < 1/3 * (1 + 4 * u1y)) {
```
Ckecks if [u2y]{colour="#FFFFFF" bg-colour="#009999"} satisfies the condition for acceptance, where the target density is proportional to $\frac{1}{3}(1+4u_1)$

```{r, eval=FALSE}
amostra.y[countn + 1] <- u1y
```
Acceptance [u1y]{colour="#FFFFFF" bg-colour="#009999"} as sample for $\mathbf{Y}$ and stores it in amostra.y

#### Start of Metropolis-Hastings for $\mathbf{Z}$

```{r, eval=FALSE}
yi <- amostra.y[countn + 1]
```
Sets [yi]{colour="#FFFFFF" bg-colour="#009999"} to the most recently accepted sample of $\mathbf{Z}$

```{r, eval=FALSE}
v <- runif(1)
amostra.z[1] <- v
```
Generates a random value $v$ uniformily distributed in $[0,1]$ and assigns it as the initial sample $z_0$ for $\mathbf{Z}$

```{r, eval=FALSE}
ui <- runif(1)
vi <- runif(1)
zimenos1 <- amostra.z[countn + 1]
```

* [ui]{colour="#FFFFFF" bg-colour="#009999"}: Uniform random number used in the acceptance step of Metropolis-Hastings

* [vi]{colour="#FFFFFF" bg-colour="#009999"}: Proposed value for $\mathbf{Z}$ in the Markov chain

* [zimenos1]{colour="#FFFFFF" bg-colour="#009999"}: Previous value of $\mathbf{Z}$

#### Evaluate Conditional Densities for $\mathbf{Z}$

```{r, eval=FALSE}
fyvi <- 2 * (vi + 2 * yi) / (1 + 4 * yi)
fyzimenos1 <- 2 * (zimenos1 + 2 * yi) / (1 + 4 * yi)
```
Calculates the conditional densities $f(y|z)\, for \, \mathbf{Z} \, = \, vi$ and $\mathbf{Z} \, = \, zimenos1$, given the current $\mathbf{Y}$

#### Uniform Proposal Distribution

```{r, eval=FALSE}
fvvi <- 1
fvzimenos1 <- 1
```
Since the proposal distribution is uniform on $[0,1]$, its density is 1

#### Calculate Acceptance Ratio and Decide
```{r, eval=FALSE}
roi <- min((fyvi / fvvi * fvzimenos1 / fyzimenos1), 1)
if (ui <= roi) {
  amostra.z[countn + 2] <- vi
} else {
  amostra.z[countn + 2] <- zimenos1
}
```

* [roi]{colour="#FFFFFF" bg-colour="#009999"}: Calculates the acceptance ratio $\rho_i$

* If $u_i \le \rho_i$, the proposal $vi$ is accepted

* Otherwise, the chain retains the previous value $zimenos1$

## Question 5
```{r}
fx_yz<-function(x,y,z){(x+y+z)/(y+z+1/2)}
fy_xz<-function(x,y,z){(x+y+z)/(x+z+1/2)}
fz_xy<-function(x,y,z){(x+y+z)/(x+y+1/2)}

gibbs <- function(n, m, burn) {
  x <- 0
  y <- 0
  z <- 0
  
  count <- 2
  
  repeat {
    if (length(z) == n) break
    
    # Gerando x
    u_x <- runif(1, 0, 1)
    u_y <- runif(1, 0, m)
    
    y_n <- fx_yz(u_x, y[count - 1], z[count - 1])
    
    if (u_y < y_n) {
      x[count] <- u_x
      
      # Gerando y
      u_x <- runif(1, 0, 1)
      u_y <- runif(1, 0, m)
      
      y_n <- fy_xz(x[count], u_x, z[count - 1])
      
      if (u_y < y_n) {
        y[count] <- u_x
        
        # Gerando z
        u_x <- runif(1, 0, 1)
        u_y <- runif(1, 0, m)
        
        y_n <- fz_xy(x[count], y[count], u_x)
        
        if (u_y < y_n) {
          z[count] <- u_x
          count <- count + 1
        }
      }
    }
  }
  
  burn_in <- ceiling(burn * n)
  x <- x[-(1:burn_in)]
  y <- y[-(1:burn_in)]
  z <- z[-(1:burn_in)]
  
  return(cbind(x, y, z))
}
```

```{r}
j <- gibbs(1000, 10, 0.1)
```

$$
E(X) = E(Y) = E(Z) = \frac{5}{9} = 0.5555556
$$

```{r}
mean(j[,1])
mean(j[,2])
mean(j[,3])
```

